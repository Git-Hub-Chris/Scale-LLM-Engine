# This is a YAML-formatted file.

# replicaCount specifies the amount of replica pods for each deployment
replicaCount:
  gateway: 2
  cacher: 1
  builder: 1
  balloonA10: 0
  balloonA100: 0
  balloonCpu: 0
  balloonT4: 0

# tag is the LLM Engine docker image tag
tag: b144dd4e5371484be1889c76e70baec375127b52
context: production
image:
  # gatewayRepository is the docker repository to pull the LLM Engine gateway image from
  gatewayRepository: public.ecr.aws/b2z8n5q1/llm-engine
  # builderRepository is the docker repository to pull the LLM Engine endpoint builder image from
  builderRepository: public.ecr.aws/b2z8n5q1/llm-engine
  # cacherRepository is the docker repository to pull the LLM Engine cacher image from
  cacherRepository: public.ecr.aws/b2z8n5q1/llm-engine
  # forwarderRepository is the docker repository to pull the LLM Engine forwarder image from
  forwarderRepository: public.ecr.aws/b2z8n5q1/llm-engine
  pullPolicy: Always

datadog_trace_enabled: false

secrets:
  # kubernetesDatabaseSecretName is the name of the secret that contains the database credentials
  kubernetesDatabaseSecretName: ml-infra-pg

service:
  type: ClusterIP
  port: 80

# TODO: This is related to istio. Maybe delete it
virtualservice:
  enabled: false
  annotations: { }
  hostDomains:
    - egp-test.scale.com
  gateways:
    - default/internal-gateway

# TODO: This is related to istio. Maybe delete it
destinationrule:
  enabled: false
  annotations: { }

autoscaling:
  horizontal:
    enabled: true
    minReplicas: 2
    maxReplicas: 10
    targetConcurrency: 50
  vertical:
    enabled: false
# TODO: Why is this commented out?
#    minAllowed:
#      cpu: 100m
#      memory: 128Mi
#    maxAllowed:
#      cpu: 10
#      memory: 8Gi
#    updateMode: Initial # Auto disrupts the service (https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md#quick-start)
  prewarming:
    enabled: false

resources:
  requests:
    cpu: 2

nodeSelector: { }

tolerations: [ ]

affinity: { }

config:
  values:
    infra:
      k8s_cluster_name: main_cluster
      dns_host_domain: egp-test.scale.com
      default_region: us-east-1
      ml_account_id: "692474966980"
      docker_repo_prefix: "692474966980.dkr.ecr.us-east-1.amazonaws.com"
      redis_host: spellbook-prod-cache-001.yoibpc.0001.use1.cache.amazonaws.com
      s3_bucket: "scale-ml-egp-test"
    llm_engine:
      # Endpoint config
      # K8s namespace the endpoints will be created in
      endpoint_namespace: llm-engine

      # Asynchronous endpoints (coming soon)
      sqs_profile: default
      sqs_queue_policy_template: >
        {
          "Version": "2012-10-17",
          "Id": "__default_policy_ID",
          "Statement": [
            {
              "Sid": "__owner_statement",
              "Effect": "Allow",
              "Principal": {
                "AWS": "arn:aws:iam::985572151633:root"
              },
              "Action": "sqs:*",
              "Resource": "arn:aws:sqs:us-east-1:985572151633:${queue_name}"
            },
            {
              "Effect": "Allow",
              "Principal": {
                "AWS": "arn:aws:iam::985572151633:role/k8s-main-llm-engine"
              },
              "Action": "sqs:*",
              "Resource": "arn:aws:sqs:us-east-1:985572151633:${queue_name}"
            }
          ]
        }

      sqs_queue_tag_template: >
        {
          "Spellbook-Serve-Endpoint-Id": "${endpoint_id}",
          "Spellbook-Serve-Endpoint-Name": "${endpoint_name}",
          "Spellbook-Serve-Endpoint-Created-By": "${endpoint_created_by}"
        }

      cache_redis_url: redis://spellbook-prod-cache-001.yoibpc.0001.use1.cache.amazonaws.com:6379/15
      s3_file_llm_fine_tuning_job_repository: "s3://scale-ml/hosted-model-inference/llm-ft-job-repository/circleci"

serviceAccount:
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::985572151633:role/k8s-main-llm-engine
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "-2"
  namespaces: []

aws:
  configMap:
    name: default-config
    create: true
  profileName: default

# Triton enhanced endpoints (coming soon)
triton:
  image:
    repository: 692474966980.dkr.ecr.us-west-2.amazonaws.com/std-ml-srv
    tag: e83eccbc8959f90ebbe4bda618b61ec6ee2d8394-triton

serviceTemplate:
  securityContext:
    capabilities:
      drop:
        - all
  mountInfraConfig: true

imageCache:
  devices:
    - name: cpu
      nodeSelector:
        cpu-only: "true"
    - name: a10
      nodeSelector:
        k8s.amazonaws.com/accelerator: nvidia-ampere-a10
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
    - name: a100
      nodeSelector:
        k8s.amazonaws.com/accelerator: nvidia-ampere-a100
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
    - name: t4
      nodeSelector:
        k8s.amazonaws.com/accelerator: nvidia-tesla-t4
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"

celeryBrokerType: sqs
