{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will demonstrate fine-tuning open source models in order to classify emails into two categories, based on their content.\n",
    "\n",
    "We will prepare 950 examples to fine-tune on, and use 50 examples to test accuracy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "categories = ['rec.sport.baseball', 'rec.sport.hockey']\n",
    "sports_dataset = fetch_20newsgroups(subset='train', shuffle=True, random_state=42, categories=categories)\n",
    "\n",
    "labels = [sports_dataset.target_names[x].split('.')[-1] for x in sports_dataset['target']]\n",
    "texts = [text.strip() for text in sports_dataset['data']]\n",
    "df = pd.DataFrame(zip(texts, labels), columns = ['raw_prompt','response'])[:1000]\n",
    "df_train = df[:950]\n",
    "df_test = df[950:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: dougb@comm.mot.com (Doug Bank)\\nSubject: Re: Info needed for Cleveland tickets\\nReply-To: dougb@ecs.comm.mot.com\\nOrganization: Motorola Land Mobile Products Sector\\nDistribution: usa\\nNntp-Posting-Host: 145.1.146.35\\nLines: 17\\n\\nIn article <1993Apr1.234031.4950@leland.Stanford.EDU>, bohnert@leland.Stanford.EDU (matthew bohnert) writes:\\n\\n|> I'm going to be in Cleveland Thursday, April 15 to Sunday, April 18.\\n|> Does anybody know if the Tribe will be in town on those dates, and\\n|> if so, who're they playing and if tickets are available?\\n\\nThe tribe will be in town from April 16 to the 19th.\\nThere are ALWAYS tickets available! (Though they are playing Toronto,\\nand many Toronto fans make the trip to Cleveland as it is easier to\\nget tickets in Cleveland than in Toronto.  Either way, I seriously\\ndoubt they will sell out until the end of the season.)\\n\\n-- \\nDoug Bank                       Private Systems Division\\ndougb@ecs.comm.mot.com          Motorola Communications Sector\\ndougb@nwu.edu                   Schaumburg, Illinois\\ndougb@casbah.acns.nwu.edu       708-576-8207\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['raw_prompt'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      "baseball    498\n",
      "hockey      452\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "response\n",
       "baseball    25\n",
       "hockey      25\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train['response'].value_counts())\n",
    "\n",
    "df_test['response'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(text: str):\n",
    "    return f\"Prompt: {text}\\nCategory: \"\n",
    "\n",
    "def prepare_df(df: pd.DataFrame):\n",
    "    df['prompt'] = df['raw_prompt'].apply(build_prompt)\n",
    "    df.drop('raw_prompt', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p5/s862844923q4g7yvtkc3s9tm0000gp/T/ipykernel_91389/2775128183.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['prompt'] = df['raw_prompt'].apply(build_prompt)\n",
      "/var/folders/p5/s862844923q4g7yvtkc3s9tm0000gp/T/ipykernel_91389/2775128183.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop('raw_prompt', axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "prepare_df(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseball</td>\n",
       "      <td>Prompt: From: dougb@comm.mot.com (Doug Bank)\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hockey</td>\n",
       "      <td>Prompt: From: gld@cunixb.cc.columbia.edu (Gary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baseball</td>\n",
       "      <td>Prompt: From: rudy@netcom.com (Rudy Wade)\\nSub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hockey</td>\n",
       "      <td>Prompt: From: monack@helium.gas.uug.arizona.ed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>baseball</td>\n",
       "      <td>Prompt: Subject: Let it be Known\\nFrom: &lt;ISSBT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   response                                             prompt\n",
       "0  baseball  Prompt: From: dougb@comm.mot.com (Doug Bank)\\n...\n",
       "1    hockey  Prompt: From: gld@cunixb.cc.columbia.edu (Gary...\n",
       "2  baseball  Prompt: From: rudy@netcom.com (Rudy Wade)\\nSub...\n",
       "3    hockey  Prompt: From: monack@helium.gas.uug.arizona.ed...\n",
       "4  baseball  Prompt: Subject: Let it be Known\\nFrom: <ISSBT..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data needs to end up in a CSV file that has two columns: `prompt` and `response`, and that is publicly accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"sports_training_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, we upload our csv to `s3://scale-demo-datasets/sports/sports_training_dataset.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to set an environment variable SCALE_API_KEY. \n",
    "#   Since we're in an ipynb, we can do this. Otherwise, you just need to set the environment variable\n",
    "import dotenv\n",
    "env_file = \".env\"\n",
    "dotenv.load_dotenv(env_file, override=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmengine import FineTune, Completion, Model\n",
    "\n",
    "FineTune.validate_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnauthorizedError",
     "evalue": "Invalid API Key.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnauthorizedError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m create_fine_tune_response \u001b[39m=\u001b[39m FineTune\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      2\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mllama-7b\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     training_file\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39ms3://scale-demo-datasets/sports/sports_training_dataset.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m     validation_file\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      5\u001b[0m     hyperparameters\u001b[39m=\u001b[39;49m{},\n\u001b[1;32m      6\u001b[0m     suffix\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmy-first-finetune\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m fine_tune_id \u001b[39m=\u001b[39m create_fine_tune_response\u001b[39m.\u001b[39mfine_tune_id\n\u001b[1;32m     12\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m            training_file (`str`):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m                Hyperparameters\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/llm-engine/clients/python/llmengine/fine_tuning.py:76\u001b[0m, in \u001b[0;36mFineTune.create\u001b[0;34m(cls, model, training_file, validation_file, hyperparameters, suffix)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39mCreates a job that fine-tunes a specified model from a given dataset.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39m    CreateFineTuneResponse: an object that contains the ID of the created fine-tuning job\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m request \u001b[39m=\u001b[39m CreateFineTuneRequest(\n\u001b[1;32m     70\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     71\u001b[0m     training_file\u001b[39m=\u001b[39mtraining_file,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     suffix\u001b[39m=\u001b[39msuffix,\n\u001b[1;32m     75\u001b[0m )\n\u001b[0;32m---> 76\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mpost_sync(\n\u001b[1;32m     77\u001b[0m     resource_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mv1/llm/fine-tunes\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     78\u001b[0m     data\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mdict(),\n\u001b[1;32m     79\u001b[0m     timeout\u001b[39m=\u001b[39;49mDEFAULT_TIMEOUT,\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     81\u001b[0m \u001b[39mreturn\u001b[39;00m CreateFineTuneResponse\u001b[39m.\u001b[39mparse_obj(response)\n",
      "File \u001b[0;32m~/Documents/llm-engine/clients/python/llmengine/api_engine.py:79\u001b[0m, in \u001b[0;36mAPIEngine.post_sync\u001b[0;34m(cls, resource_name, data, timeout)\u001b[0m\n\u001b[1;32m     72\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(\n\u001b[1;32m     73\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(LLM_ENGINE_BASE_PATH, resource_name),\n\u001b[1;32m     74\u001b[0m     json\u001b[39m=\u001b[39mdata,\n\u001b[1;32m     75\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m     76\u001b[0m     headers\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mx-api-key\u001b[39m\u001b[39m\"\u001b[39m: api_key},\n\u001b[1;32m     77\u001b[0m )\n\u001b[1;32m     78\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[39mraise\u001b[39;00m parse_error(response\u001b[39m.\u001b[39mstatus_code, response\u001b[39m.\u001b[39mcontent)\n\u001b[1;32m     80\u001b[0m payload \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n\u001b[1;32m     81\u001b[0m \u001b[39mreturn\u001b[39;00m payload\n",
      "\u001b[0;31mUnauthorizedError\u001b[0m: Invalid API Key."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "create_fine_tune_response = FineTune.create(\n",
    "    model=\"llama-7b\",\n",
    "    training_file=\"s3://scale-demo-datasets/sports/sports_training_dataset.csv\",\n",
    "    validation_file=None,\n",
    "    hyperparameters={},\n",
    "    suffix=\"my-first-finetune\"\n",
    ")\n",
    "\n",
    "fine_tune_id = create_fine_tune_response.fine_tune_id\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Args:\n",
    "            training_file (`str`):\n",
    "                A path to the file containing the training dataset.\n",
    "                Dataset must be a CSV file with columns 'prompt' and 'response'.\n",
    "                The value must be the URI of a publicly accessible file on bulk storage, e.g.\n",
    "                s3://{public_s3_bucket}/{public_s3_key} for a file stored on s3.\n",
    "            validation_file (`str`):\n",
    "                A path to the file containing the validation dataset.\n",
    "                Has the same format as training_file.\n",
    "                If not provided, we will generate a split from the training dataset.\n",
    "            model_name (`str`):\n",
    "                The name of the fine-tuned model\n",
    "            base_model (`str`):\n",
    "                Base model to train from\n",
    "            fine_tuning_method (`str`):\n",
    "                Fine-tuning method\n",
    "            hyperparameters (`str`):\n",
    "                Hyperparameters\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fine_tune_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     fine_tune_status \u001b[39m=\u001b[39m FineTune\u001b[39m.\u001b[39mretrieve(fine_tune_id)\u001b[39m.\u001b[39mstatus\n\u001b[1;32m      6\u001b[0m     \u001b[39mif\u001b[39;00m fine_tune_status \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSUCCESS\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m      7\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fine_tune_id' is not defined"
     ]
    }
   ],
   "source": [
    "# Wait for fine tune to complete\n",
    "\n",
    "import time\n",
    "while True:\n",
    "    fine_tune_status = FineTune.retrieve(fine_tune_id).status\n",
    "    if fine_tune_status == \"SUCCESS\":\n",
    "        break\n",
    "    elif fine_tune_status in [\"FAILURE\", \"CANCELLED\"]:\n",
    "        raise ValueError(\"Fine Tune failed\")\n",
    "    time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_models \u001b[39m=\u001b[39m Model\u001b[39m.\u001b[39;49mlist()\u001b[39m.\u001b[39mmodel_endpoints\n\u001b[1;32m      3\u001b[0m \u001b[39m# We want to get just your fine-tuned models.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m your_personal_fine_tunes \u001b[39m=\u001b[39m [model \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m all_models \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model\u001b[39m.\u001b[39mspec\u001b[39m.\u001b[39mpublic_inference]\n",
      "File \u001b[0;32m~/Documents/llm-engine/clients/python/llmengine/model.py:76\u001b[0m, in \u001b[0;36mModel.list\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mcls\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ListLLMModelEndpointsV1Response:\n\u001b[1;32m     70\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    List model endpoints\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \n\u001b[1;32m     73\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39m        response: list of model endpoints\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mv1/model-endpoints\u001b[39;49m\u001b[39m\"\u001b[39;49m, timeout\u001b[39m=\u001b[39;49mDEFAULT_TIMEOUT)\n\u001b[1;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m ListLLMModelEndpointsV1Response\u001b[39m.\u001b[39mparse_obj(response)\n",
      "File \u001b[0;32m~/Documents/llm-engine/clients/python/llmengine/api_engine.py:49\u001b[0m, in \u001b[0;36mAPIEngine.get\u001b[0;34m(cls, resource_name, timeout)\u001b[0m\n\u001b[1;32m     43\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(\n\u001b[1;32m     44\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(LLM_ENGINE_BASE_PATH, resource_name),\n\u001b[1;32m     45\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m     46\u001b[0m     headers\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mx-api-key\u001b[39m\u001b[39m\"\u001b[39m: api_key},\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mraise\u001b[39;00m parse_error(response\u001b[39m.\u001b[39mstatus_code, response\u001b[39m.\u001b[39mcontent)\n\u001b[1;32m     50\u001b[0m payload \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n\u001b[1;32m     51\u001b[0m \u001b[39mreturn\u001b[39;00m payload\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Not Found"
     ]
    }
   ],
   "source": [
    "all_models = Model.list().model_endpoints\n",
    "\n",
    "# We want to get just your fine-tuned models.\n",
    "your_personal_fine_tunes = [model for model in all_models if not model.spec.public_inference]\n",
    "\n",
    "your_fine_tuned_model = your_personal_fine_tunes[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = Completion.create(model_name=your_fine_tuned_model, prompt=\"TODO\", max_new_tokens=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
