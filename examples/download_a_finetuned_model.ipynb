{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8d3a4214",
      "metadata": {
        "id": "8d3a4214"
      },
      "source": [
        "# Downlaod a FineTuned Model \n",
        "This notebook demonstrates how to download a finetuned model that you've created using LLM Engine and add it to huggingface!\n",
        "\n",
        "**This notebook is an extension of the previous finetuning notebook on ScienceQA**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XK6VpTnOL4OV",
      "metadata": {
        "id": "XK6VpTnOL4OV"
      },
      "source": [
        "# Packages Required\n",
        "For this demo, we'll be using the `scale-llm-engine` package, the `datasets` package for downloading our finetuning dataset, `transformers`, and `huggingface_hub` for uploading our model to huggingface.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S5u6DdInMEQ7",
      "metadata": {
        "id": "S5u6DdInMEQ7"
      },
      "outputs": [],
      "source": [
        "!pip install scale-llm-engine\n",
        "!pip install transformers\n",
        "!pip install huggingface_hub\n",
        "!pip install datasets\n",
        "!pip install aiohttp   "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3dc2a56",
      "metadata": {
        "id": "a3dc2a56"
      },
      "source": [
        "# Data Preparation\n",
        "Let's load in the dataset using Huggingface and view the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e06ac39e",
      "metadata": {
        "id": "e06ac39e"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from smart_open import smart_open\n",
        "import pandas as pd\n",
        "\n",
        "dataset = load_dataset('derek-thomas/ScienceQA')\n",
        "dataset['train'].features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cbe8a58",
      "metadata": {
        "id": "1cbe8a58"
      },
      "source": [
        "Now, let's format the dataset into what's acceptable for LLM Engine - a CSV file with 'prompt' and 'response' columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b0eb8ad",
      "metadata": {
        "id": "0b0eb8ad"
      },
      "outputs": [],
      "source": [
        "choice_prefixes = [chr(ord('A') + i) for i in range(26)] # A-Z\n",
        "def format_options(options, choice_prefixes):\n",
        "    return ' '.join([f'({c}) {o}' for c, o in zip(choice_prefixes, options)])\n",
        "\n",
        "def format_prompt(r, choice_prefixes):\n",
        "    options = format_options(r['choices'], choice_prefixes)\n",
        "    return f'''Context: {r[\"hint\"]}\\nQuestion: {r[\"question\"]}\\nOptions:{options}\\nAnswer:'''\n",
        "\n",
        "def format_label(r, choice_prefixes):\n",
        "    return choice_prefixes[r['answer']]\n",
        "\n",
        "def convert_dataset(ds):\n",
        "    prompts = [format_prompt(i, choice_prefixes) for i in ds if i['hint'] != '']\n",
        "    labels = [format_label(i, choice_prefixes) for i in ds if i['hint'] != '']\n",
        "    df = pd.DataFrame.from_dict({'prompt': prompts, 'response': labels})\n",
        "    return df\n",
        "\n",
        "save_to_s3 = False\n",
        "df_train = convert_dataset(dataset['train'])\n",
        "if save_to_s3:\n",
        "    train_url = 's3://...'\n",
        "    val_url = 's3://...'\n",
        "    df_train = convert_dataset(dataset['train'])\n",
        "    with smart_open(train_url, 'wb') as f:\n",
        "        df_train.to_csv(f)\n",
        "\n",
        "    df_val = convert_dataset(dataset['validation'])\n",
        "    with smart_open(val_url, 'wb') as f:\n",
        "        df_val.to_csv(f)\n",
        "else:\n",
        "    # Gists of the already processed datasets\n",
        "    train_url = 'https://gist.githubusercontent.com/jihan-yin/43f19a86d35bf22fa3551d2806e478ec/raw/91416c09f09d3fca974f81d1f766dd4cadb29789/scienceqa_train.csv'\n",
        "    val_url = 'https://gist.githubusercontent.com/jihan-yin/43f19a86d35bf22fa3551d2806e478ec/raw/91416c09f09d3fca974f81d1f766dd4cadb29789/scienceqa_val.csv'\n",
        "\n",
        "df_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2fc8d76",
      "metadata": {
        "id": "e2fc8d76"
      },
      "source": [
        "# Fine-tune\n",
        "Now, we can fine-tune the model using LLM Engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4905d447",
      "metadata": {
        "id": "4905d447"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['SCALE_API_KEY'] = 'xxx'\n",
        "\n",
        "from llmengine import FineTune\n",
        "\n",
        "response = FineTune.create(\n",
        "    model=\"llama-2-7b\",\n",
        "    training_file=train_url,\n",
        "    validation_file=val_url,\n",
        "    hyperparameters={\n",
        "        'lr':2e-4,\n",
        "    },\n",
        "    suffix='science-qa-llama'\n",
        ")\n",
        "run_id = response.id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55074457",
      "metadata": {
        "id": "55074457"
      },
      "source": [
        "We can sleep until the job completes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "840938dd",
      "metadata": {
        "id": "840938dd"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "while True:\n",
        "    job_status = FineTune.get(run_id).status\n",
        "    print(job_status)\n",
        "    if job_status == 'SUCCESS':\n",
        "        break\n",
        "    time.sleep(60)\n",
        "\n",
        "fine_tuned_model = FineTune.get(run_id).fine_tuned_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31278c6d",
      "metadata": {
        "id": "31278c6d"
      },
      "source": [
        "# Downloading our Finetuned model \n",
        "Let's download the weights for the new fine-tuned model using LLM Engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f2f3f43",
      "metadata": {
        "id": "9f2f3f43"
      },
      "outputs": [],
      "source": [
        "from llmengine import Model\n",
        "\n",
        "response = Model.download(FineTune.get(run_id).fine_tune_model, download_format=\"huggingface\")\n",
        "print(response.urls)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae9cbdf3",
      "metadata": {},
      "source": [
        "We now have a list of urls that point to the file(s) where our finetuned model lives. We can download the associated finetuned model either synchronously or asynchronously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc363e48",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import aiohttp\n",
        "import asyncio\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "async def download_file(session, url, output_dir):\n",
        "    # Parse the URL to get the filename\n",
        "    parsed = urlparse(url)\n",
        "    filename = os.path.basename(parsed.path)\n",
        "\n",
        "    # Download the file\n",
        "    async with session.get(url) as response:\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Write the file\n",
        "        with open(os.path.join(output_dir, filename), 'wb') as f:\n",
        "            f.write(await response.read())\n",
        "\n",
        "        print(f\"Downloaded {filename}\")\n",
        "\n",
        "async def download_files(urls, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = []\n",
        "        for url in urls:\n",
        "            tasks.append(download_file(session, url, output_dir))\n",
        "\n",
        "        await asyncio.gather(*tasks)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "000e1633",
      "metadata": {},
      "outputs": [],
      "source": [
        "output_directory = \"YOUR_MODEL_DIR\"\n",
        "asyncio.run(download_files(response.urls, output_directory))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "328efd19",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "from getpass import getpass\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "def create_tarball(source_dir, output_filename):\n",
        "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
        "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
        "\n",
        "def upload_to_huggingface_model_hub(source_dir, model_name, hf_username, hf_password=None):\n",
        "    # Get password if not provided\n",
        "    if hf_password is None:\n",
        "        hf_password = getpass(\"Enter Hugging Face password: \")\n",
        "\n",
        "    # Log in to Hugging Face\n",
        "    api = HfApi()\n",
        "    token = api.login(hf_username, hf_password)\n",
        "\n",
        "    # Create a new repository\n",
        "    print(f\"Creating new model {model_name}.\")\n",
        "    repo_url = api.create_repo(token, model_name, exist_ok=True)\n",
        "\n",
        "    # Create a tarball of the source directory\n",
        "    tarball_name = f\"{model_name}.tar.gz\"\n",
        "    create_tarball(source_dir, tarball_name)\n",
        "\n",
        "    # Upload the tarball\n",
        "    print(f\"Uploading {tarball_name} to {repo_url}.\")\n",
        "    HfApi().upload_file(\n",
        "        path_or_fileobj=tarball_name,\n",
        "        path_in_repo=tarball_name,  # The name of the file in the repo\n",
        "        repo_id=f\"{hf_username}/{model_name}\",  # The id of the repo\n",
        "        token=token,\n",
        "    )\n",
        "\n",
        "    # Delete the tarball\n",
        "    os.remove(tarball_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de55e438",
      "metadata": {},
      "outputs": [],
      "source": [
        "upload_to_huggingface_model_hub('YOUR_MODEL_DIR', 'YOUR_MODEL_NAME', 'YOUR_HUGGINGFACE_USER_NAME', 'YOUR_HUGGINGFACE_PASSWORD')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Environment (conda_pytorch_p38)",
      "language": "python",
      "name": "conda_pytorch_p38"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
