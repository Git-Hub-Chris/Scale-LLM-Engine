FROM nvcr.io/nvidia/pytorch:23.09-py3

RUN apt-get update && \
    apt-get install -y dumb-init psmisc && \
    apt-get autoremove -y && \
    rm -rf /var/lib/apt/lists/* && \
    apt-get clean

RUN pip uninstall torch -y
RUN pip install torch==2.1.1 --index-url https://download.pytorch.org/whl/cu121

RUN pip uninstall xformers -y
RUN pip install xformers==0.0.23 --index-url https://download.pytorch.org/whl/cu121

# Need to fix flash-infer at 0.0.8 to support gemma models
# See https://github.com/vllm-project/vllm/issues/7060#issuecomment-2266248014
RUN pip uninstall flash-infer -y
RUN pip install flash-infer==0.0.8 --index-url -i https://flashinfer.ai/whl/cu121/torch2.1/

RUN wget https://github.com/peak/s5cmd/releases/download/v2.2.1/s5cmd_2.2.1_Linux-64bit.tar.gz
RUN tar -xvzf s5cmd_2.2.1_Linux-64bit.tar.gz

COPY model-engine/model_engine_server/inference/batch_inference/requirements.txt /workspace/requirements.txt
RUN pip install -r requirements.txt

COPY model-engine /workspace/model-engine
RUN pip install -e /workspace/model-engine
COPY model-engine/model_engine_server/inference/batch_inference/vllm_batch.py /workspace/vllm_batch.py
