# Config for scale-hosted Hosted Model Inference in the training cluster, plus a bunch of other config-ish notes
# NOTE: If you add/change values inside this file that need to apply to all clusters, please make changes in
# service_config_prod.yaml + service_config_staging.yaml as well.
model_primitive_host: "none"

# Endpoint config
# K8s namespace the endpoints will be created in
endpoint_namespace: scale-deploy

forwarder_image_tag: fb01f90a15f2826792d75c0ae0eaefa4215eb975-cpu

# Asynchronous endpoints
# TODO: Try out localstack once e2e tests have been updated to use sqs as a broker_type
sqs_profile: nonexistent_sqs_profile
sqs_queue_policy_template: >
  {
    "Version": "2012-10-17",
    "Id": "__default_policy_ID",
    "Statement": [
      {
        "Sid": "__owner_statement",
        "Effect": "Allow",
        "Principal": {
          "AWS": "arn:aws:iam::692474966980:root"
        },
        "Action": "sqs:*",
        "Resource": "arn:aws:sqs:us-west-2:692474966980:${queue_name}"
      },
      {
        "Effect": "Allow",
        "Principal": {
          "AWS": "arn:aws:iam::307185671274:role/ml-worker"
        },
        "Action": "sqs:*",
        "Resource": "arn:aws:sqs:us-west-2:692474966980:${queue_name}"
      },
      {
        "Effect": "Allow",
        "Principal": {
          "AWS": "arn:aws:iam::307185671274:role/ml_spellbook_serve"
        },
        "Action": "sqs:*",
        "Resource": "arn:aws:sqs:us-west-2:692474966980:${queue_name}"
      }
    ]
  }

sqs_queue_tag_template: >
  {
    "infra.scale.com/product": "MLInfraLaunchSQS",
    "infra.scale.com/team": "${team}",
    "infra.scale.com/contact": "yi.xu@scale.com",
    "infra.scale.com/customer": "AllCustomers",
    "infra.scale.com/financialOwner": "yi.xu@scale.com",
    "Launch-Endpoint-Id": "${endpoint_id}",
    "Launch-Endpoint-Name": "${endpoint_name}",
    "Launch-Endpoint-Created-By": "${endpoint_created_by}"
  }

# resultsS3Bucket (i.e. where HMI will store model inference results) is currently determined on endpoint creation
# via a request

# modelBundleS3Bucket (i.e. where model bundles are stored) is not determined by any HMI code, but instead
# by some scaleapi routing layer code for scale-hosted HMI, and by request parameters in general.

# Currently, the celery redis used is defaulted to scale's celery redis, and is hardcoded inside scaleml's celery impl.
# We'll need to bundle this celery implementation along for open-source hosting.

# There's a separate piece of infra that caches k8s state onto redis, so we need a url to it
cache_redis_url: redis://127.0.0.1:6379/15
