{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u26a1 Spellbook Serve \u26a1","text":"<p>The fastest, cheapest, and easiest way to deploy and scale your foundation models.</p>"},{"location":"#quick-install","title":"\ud83d\udcbb Quick Install","text":"Install using pipInstall using conda <pre><code>pip install spellbook-serve\n</code></pre> <pre><code>conda install spellbook-serve -c conda-forge\n</code></pre>"},{"location":"#about","title":"\ud83e\udd14 About","text":"<p>Foundation models are emerging as the building blocks of AI. However, deploying these models to the cloud still requires infrastructure expertise, and can be expensive.</p> <p>Spellbook Serve is a Python library, CLI, and Helm chart that provides everything you need to deploy your foundation models to the cloud using Kubernetes. Key features include:</p> <p>\ud83e\udd17 Open-Source Integrations: Deploy any Huggingface model with a single command. Integrate seamlessly with Langchain chat applications.</p> <ul> <li>Huggingface Documentation</li> <li>Huggingface Example</li> <li>Langchain Documentation</li> <li>Langchain Example</li> </ul> <p>\u2744 Fast Cold-Start Times: To prevent GPUs from idling, Spellbook Serve automatically scales your model to zero when it's not in use and scales up within seconds, even for large foundation models.</p> <ul> <li>Documentation</li> <li>Benchmarks</li> </ul> <p>\ud83d\udcb8 Cost-Optimized: Deploy AI models up to 7x cheaper than OpenAI APIs, including cold-start and warm-down times.</p> <ul> <li>Benchmarks</li> </ul> <p>\ud83d\udc33 Deploying from any docker image: Turn any Docker image into an auto-scaling deployment with simple APIs.</p> <ul> <li>Documentation</li> <li>Example</li> </ul> <p>\ud83c\udf99\ufe0f Language-Model Specific Features: Spellbook Serve provides APIs for streaming responses and dynamically batching inputs for higher throughput and lower latency.</p> <ul> <li>Streaming Documentation</li> <li>Streaming Example</li> <li>Dynamic Batching Documentation</li> <li>Dynamic Batching Example</li> </ul>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"getting_started/","title":"\ud83d\ude80 Getting Started","text":"<p>To start using Spellbook Serve with public inference APIs, simply run the following:</p> Install using pipInstall using conda <pre><code>pip install spellbook-serve\n</code></pre> <pre><code>conda install spellbook-serve -c conda-forge\n</code></pre> <p>You can generate an API key using the CLI command</p> <pre><code>spellbook-serve generate-api-key\n</code></pre> <p>With the API key, you can now send requests to Spellbook Serve public inference APIs using the CLI or Python client:</p> Using the CLIUsing the Python Client <pre><code>spellbook-serve endpoints list\n\n# Expected output:\n#\n# public-flan-t5-xxl\n# public-stablelm-2\n# public-dolly-2\n\nspellbook-serve endpoints send public-flan-t5-xxl \\\n    --input \"Hello, my name is\"\n    --temperature 0.5\n    --top_p 0.9\n\n# Expected output:\n#\n# Hello, my name is Flan.\n</code></pre> <pre><code>import spellbook_serve as ss\nclient = ss.Client()\nrequest = ss.EndpointRequest(\ninput=\"Hello, my name is\",\ntemperature=0.5,\ntop_p=0.9,\n)\nendpoint = client.get_model_endpoint(\"public-flan-t5-xxl\")\nfuture = endpoint.send(request)\nresponse = future.get()\nprint(response)\n</code></pre>"},{"location":"getting_started/#installation-on-kubernetes","title":"\ud83d\udcbb Installation on Kubernetes","text":"<p>To install Spellbook Serve on Kubernetes, you can use the Helm chart:</p> <pre><code>helm repo add spellbook https://spellbook.github.io/helm-charts\nhelm repo update\nhelm install spellbook-serve spellbook/spellbook-serve\n</code></pre>"},{"location":"benchmarks/cold-start-times/","title":"\u2744\ufe0f\u23f0 Cold Start Times Benchmarks","text":"<p>Here are some benchmarks for cold start times for various models:</p> Model Name Cold Start Time (mean \u00b1 std) GPT-J 15.7s \u00b1 1.3s FLAN-T5 XXL 25.7s \u00b1 1.3s Stable Diffusion 2.0 15.7s \u00b1 1.3s"},{"location":"benchmarks/cost/","title":"\ud83d\udcb8 Cost Benchmarks","text":"<p>Here are some benchmarks for cost for various models:</p>"},{"location":"concepts/dynamic-batching/","title":"Dynamic Request Batching","text":""},{"location":"concepts/streaming/","title":"Streaming","text":""},{"location":"integrations/huggingface/","title":"\ud83e\udd17 Using Spellbook Serve with Huggingface Transformers","text":"<p>To deploy huggingface models with Spellbook Serve, you need to install the <code>huggingface</code> extra:</p> <pre><code>pip install \"spellbook-serve[transformers]\"\n</code></pre> <p>To create a Model Bundle from a model on Huggingface transformers, you can use</p> <pre><code>import spellbook_serve as ss\nclient = ss.Client()\nclient.create_model_endpoint_from_huggingface(\nmodel_name=\"bert-base-uncased\",\nmodel_class=\"BertForSequenceClassification\",\ntokenizer_class=\"BertTokenizer\",\ntokenizer_name=\"bert-base-uncased\",\ntask=\"text-classification\",\nnum_labels=2,\nmax_length=128,\nbatch_size=32,\nframework=\"pt\",\nmodel_endpoint_type=\"async\",\n)\n</code></pre> <p>The input arguments to <code>create_model_endpoint_from_huggingface</code> are the same as the input arguments to the associated Huggingface pipeline.</p>"},{"location":"integrations/langchain/","title":"\ud83d\udd17 Langchain","text":"<p>There exists an Spellbook Serve LLM wrapper, which you can access with  <pre><code>from langchain.llms import SpellbookServe\n</code></pre></p> <p>The following is an example of how to use the wrapper to create a ChatGPT clone: <pre><code>from langchain import ConversationChain, LLMChain, PromptTemplate, SpellbookServe\nfrom langchain.memory import ConversationBufferWindowMemory\ntemplate = \"\"\"Assistant is a large language model.\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\nOverall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n{history}\nHuman: {human_input}\nAssistant:\"\"\"\nprompt = PromptTemplate(\ninput_variables=[\"history\", \"human_input\"], \ntemplate=template\n)\nchat_chain = LLMChain(\nllm=SpellbookServe(api_key=\"YOUR_API_KEY\", model_endpoint_name=\"public-flan-t5-xxl\"),\nprompt=prompt, \nverbose=True, \nmemory=ConversationBufferWindowMemory(k=2),\n)\noutput = chat_chain.predict(human_input=\"I want you to act as a Linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. Do not write explanations. Do not type commands unless I instruct you to do so. When I need to tell you something in English I will do so by putting text inside curly brackets {like this}. My first command is pwd.\")\nprint(output)\n</code></pre></p>"}]}