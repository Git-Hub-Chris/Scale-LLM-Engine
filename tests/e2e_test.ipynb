{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'model_endpoints': []}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "response = requests.get(\n",
    "    f\"http://localhost:5000/v1/llm/model-endpoints\",\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    auth=requests.auth.HTTPBasicAuth(os.getenv(\"LAUNCH_API_KEY\"), \"\"),\n",
    "    # timeout=DEFAULT_NETWORK_TIMEOUT_SEC,\n",
    ")\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from launch_internal import get_launch_client  # type: ignore\n",
    "\n",
    "\n",
    "class MyRequestSchema(BaseModel):\n",
    "    x: int\n",
    "    y: str\n",
    "\n",
    "\n",
    "class MyResponseSchema(BaseModel):\n",
    "    __root__: int\n",
    "\n",
    "\n",
    "api_key = os.getenv(\"LAUNCH_API_KEY\")\n",
    "env = \"training\"\n",
    "gateway_endpoint = \"http://localhost:5000\"\n",
    "launch_client = get_launch_client(api_key=api_key, env=env, gateway_endpoint=gateway_endpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListModelBundlesV2Response(model_bundles=[ModelBundleV2Response(id='bun_ciqp58jqqncg03vld5u0', name='test-streaming-bundle', metadata={}, created_at=datetime.datetime(2023, 7, 17, 19, 12, 34, 640168, tzinfo=datetime.timezone.utc), model_artifact_ids=[], schema_location='s3://scale-ml/scale-launch/model_bundles/646c085a6b38de00808148bb/705ac616-59df-4382-8c69-83accb8c0bd8', flavor=StreamingEnhancedRunnableImageFlavor(repository='ghcr.io/huggingface/text-generation-inference', tag='0.9.1', command=['text-generation-launcher', '--model-id', 'tiiuae/falcon-7b', '--num-shard', '4', '--port', '5005', '--hostname', '::'], env={'TEST_KEY': 'test_value'}, protocol='http', readiness_initial_delay_seconds=30, flavor=<ModelBundleFlavorType.STREAMING_ENHANCED_RUNNABLE_IMAGE: 'streaming_enhanced_runnable_image'>, streaming_command=['text-generation-launcher', '--model-id', 'tiiuae/falcon-7b', '--num-shard', '4', '--port', '5005', '--hostname', '::']))])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "launch_client.list_model_bundles_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SyncEndpoint(name='test-wsong', bundle_name='test-streaming-bundle', status='READY', resource_state='{\"cpus\": \"32\", \"gpus\": 4, \"memory\": \"185Gi\", \"gpu_type\": \"nvidia-ampere-a10\", \"storage\": \"200G\", \"optimize_costs\": false}', deployment_state='{\"min_workers\": 1, \"max_workers\": 3, \"per_worker\": 10, \"available_workers\": 0, \"unavailable_workers\": 1}', endpoint_type='sync', metadata='{}')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "launch_client.list_model_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "launch_client.delete_model_endpoint(\"test-wsong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRequestSchema(BaseModel):\n",
    "    x: int\n",
    "    y: str\n",
    "\n",
    "class MyResponseSchema(BaseModel):\n",
    "    __root__: int\n",
    "\n",
    "\n",
    "def my_load_predict_fn(model):\n",
    "    def returns_model_of_x_plus_len_of_y(x: int, y: str) -> int:\n",
    "        \"\"\"MyRequestSchema -> MyResponseSchema\"\"\"\n",
    "        assert isinstance(x, int) and isinstance(y, str)\n",
    "        return model(x) + len(y)\n",
    "\n",
    "    return returns_model_of_x_plus_len_of_y\n",
    "\n",
    "\n",
    "def my_load_model_fn():\n",
    "    def my_model(x):\n",
    "        return x * 2\n",
    "\n",
    "    return my_model\n",
    "\n",
    "BUNDLE_PARAMS = {\n",
    "    \"model_bundle_name\": \"test-bundle\",\n",
    "    \"load_model_fn\": my_load_model_fn,\n",
    "    \"load_predict_fn\": my_load_predict_fn,\n",
    "    \"request_schema\": MyRequestSchema,\n",
    "    \"response_schema\": MyResponseSchema,\n",
    "    \"requirements\": [\"pytest==7.2.1\", \"numpy\"],  # list your requirements here\n",
    "    \"pytorch_image_tag\": \"1.7.1-cuda11.0-cudnn8-runtime\",\n",
    "}\n",
    "\n",
    "client = launch_client(api_key=os.getenv(\"LAUNCH_API_KEY\"))\n",
    "client.create_model_bundle_from_callable_v2(**BUNDLE_PARAMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_name = \"test-streaming-bundle\"\n",
    "hf_model_name = \"tiiuae/falcon-7b\"\n",
    "num_shards = 4\n",
    "command = [\n",
    "    \"text-generation-launcher\",\n",
    "    \"--model-id\",\n",
    "    hf_model_name,\n",
    "    \"--num-shard\",\n",
    "    str(num_shards),\n",
    "    \"--port\",\n",
    "    \"5005\",\n",
    "    \"--hostname\",\n",
    "    \"::\",\n",
    "]\n",
    "\n",
    "BUNDLE_PARAMS = {\n",
    "    \"model_bundle_name\": bundle_name,\n",
    "    \"request_schema\": MyRequestSchema,\n",
    "    \"response_schema\": MyResponseSchema,\n",
    "    \"repository\": \"ghcr.io/huggingface/text-generation-inference\",\n",
    "    \"tag\": \"0.9.1\",\n",
    "    \"command\": command,  # optional; if provided, will also expose the /predict endpoint\n",
    "    \"predict_route\": \"/predict\",\n",
    "    \"healthcheck_route\": \"/readyz\",\n",
    "    \"streaming_command\": command,  # required\n",
    "    \"streaming_predict_route\": \"/stream\",\n",
    "    \"env\": {\n",
    "        \"TEST_KEY\": \"test_value\",\n",
    "    },\n",
    "    \"readiness_initial_delay_seconds\": 30,\n",
    "}\n",
    "\n",
    "# client = LaunchClient(api_key=os.getenv(\"LAUNCH_API_KEY\"), endpoint='https://launch.ml-internal.scale.com')\n",
    "# print(\"submitting request\")\n",
    "# resp = launch_client.create_model_bundle_from_streaming_enhanced_runnable_image_v2(**BUNDLE_PARAMS)\n",
    "# print(\"got resp\")\n",
    "# print(resp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating endpoint with payload:\n",
      "{'endpoint_name': 'test-wsong', 'endpoint_type': 'sync', 'update_if_exists': True, 'model_bundle': 'test-streaming-bundle', 'min_workers': 1, 'max_workers': 3, 'per_worker': 10, 'memory': '60Gi', 'storage': '20G', 'cpus': 32, 'gpus': 4, 'gpu_type': 'nvidia-ampere-a10', 'labels': {'team': 'infra', 'product': 'llm_model_zoo'}}\n",
      "Created endpoint: SyncEndpoint <endpoint_name:test-wsong>\n"
     ]
    }
   ],
   "source": [
    "endpoint_payload = {\n",
    "    \"endpoint_name\": \"test-wsong\",\n",
    "    \"endpoint_type\": \"sync\",\n",
    "    \"update_if_exists\": True,\n",
    "    \"model_bundle\": bundle_name,\n",
    "    \"min_workers\": 1,\n",
    "    \"max_workers\": 3,\n",
    "    \"per_worker\": 10,\n",
    "    \"memory\": \"60Gi\",\n",
    "    \"storage\": \"20G\",\n",
    "    \"cpus\": 32,\n",
    "    \"gpus\": 4,\n",
    "    \"gpu_type\": \"nvidia-ampere-a10\",\n",
    "    \"labels\": {\n",
    "        \"team\": \"infra\",\n",
    "        \"product\": \"llm_model_zoo\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"Creating endpoint with payload:\\n{endpoint_payload}\")\n",
    "endpoint = launch_client.create_model_endpoint(**endpoint_payload)\n",
    "print(f\"Created endpoint: {endpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncEndpoint <endpoint_name:test-wsong-2>\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "launch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
